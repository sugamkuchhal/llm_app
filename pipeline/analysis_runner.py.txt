# pipeline/analysis_runner.py

import json
import logging
import hashlib
import time
import uuid
import re

from validators.answer_validator import validate_answer
from validators.manifest_validator import validate_manifest
from charts.builder import build_bq_style_chart
from domain.barc.barc_filters import build_filters
from domain.barc.barc_rules import has_dead_hours_filter

logger = logging.getLogger("logger")


DEAD_HOURS = ("00", "01", "02", "03", "04", "05")

_INCLUDE_DEAD_HOURS_PATTERNS = (
    r"\binclude\s+dead\s+hours\b",
    r"\bincluding\s+dead\s+hours\b",
    r"\bwith\s+dead\s+hours\b",
    r"\bconsider\s+dead\s+hours\b",
    r"\ball\s+hours\b",
    r"\b24\s*x\s*7\b",
    r"\b24\s*\/\s*7\b",
    r"\b24x7\b",
)

_EXCLUDE_DEAD_HOURS_PATTERNS = (
    r"\bexclude\s+dead\s+hours\b",
    r"\bwithout\s+dead\s+hours\b",
    r"\bremove\s+dead\s+hours\b",
)


def should_exclude_dead_hours(question: str | None) -> bool:
    """
    Domain rule: exclude dead hours by default unless the user explicitly asks
    to include them.
    """
    t = (question or "").lower()

    if any(re.search(p, t) for p in _EXCLUDE_DEAD_HOURS_PATTERNS):
        return True

    if any(re.search(p, t) for p in _INCLUDE_DEAD_HOURS_PATTERNS):
        return False

    # If user mentions dead hours without "include", default remains exclusion.
    return True


def _pick_time_band_half_hour_ref(sql: str) -> str:
    """
    Prefer a qualified reference (e.g. t.time_band_half_hour) if present.
    """
    m = re.search(r"\b([A-Za-z_][A-Za-z0-9_]*)\s*\.\s*time_band_half_hour\b", sql)
    if m:
        return f"{m.group(1)}.time_band_half_hour"
    return "time_band_half_hour"


def _has_dead_hours_predicate(sql: str) -> bool:
    """
    Best-effort detection of an existing dead-hours predicate, to avoid double injection.
    """
    s = sql.lower()
    if "time_band_half_hour" not in s:
        return False

    # Covers common BigQuery forms: LEFT(col,2) [NOT] IN (...) / SUBSTR(col,1,2) [NOT] IN (...)
    return bool(
        re.search(r"\bleft\s*\(\s*[\w\.]*time_band_half_hour\s*,\s*2\s*\)\s*(not\s+)?in\s*\(", s)
        or re.search(r"\bsubstr\s*\(\s*[\w\.]*time_band_half_hour\s*,\s*1\s*,\s*2\s*\)\s*(not\s+)?in\s*\(", s)
        or re.search(r"\bsubstring\s*\(\s*[\w\.]*time_band_half_hour\s*,\s*1\s*,\s*2\s*\)\s*(not\s+)?in\s*\(", s)
    )


def apply_dead_hours_filter(sql: str) -> str:
    """
    Inject dead-hours exclusion into the OUTERMOST query only.

    Notes:
    - Applies only when the query references `time_band_half_hour`.
    - Skips injection if a dead-hours predicate already exists.
    - String-based injection is heuristic; prefer generating the predicate in SQL upstream.
    """
    if not re.search(r"\btime_band_half_hour\b", sql, flags=re.IGNORECASE):
        return sql

    if _has_dead_hours_predicate(sql):
        return sql

    col_ref = _pick_time_band_half_hour_ref(sql)
    hours = ",".join([f"'{h}'" for h in DEAD_HOURS])
    condition = f"LEFT({col_ref}, 2) NOT IN ({hours})"

    sql_lower = sql.lower()

    # Find the LAST occurrence of FROM (outermost query)
    from_idx = sql_lower.rfind(" from ")
    if from_idx == -1:
        return sql  # fail-safe: do nothing

    # Find clauses AFTER FROM
    tail = sql[from_idx:]
    tail_lower = tail.lower()

    clause_keywords = [" group by ", " order by ", " having ", " qualify ", " limit "]

    where_rel = tail_lower.find(" where ")
    if where_rel != -1:
        # Insert AND <condition> before the next clause after WHERE (or end of SQL)
        after_where_rel = where_rel + len(" where ")
        after_where = tail_lower[after_where_rel:]

        next_clause_rel = None
        for kw in clause_keywords:
            idx = after_where.find(kw)
            if idx != -1 and (next_clause_rel is None or idx < next_clause_rel):
                next_clause_rel = idx

        if next_clause_rel is None:
            insert_pos = len(sql)
        else:
            insert_pos = from_idx + after_where_rel + next_clause_rel

        before = sql[:insert_pos].rstrip()
        after = sql[insert_pos:].lstrip()
        return f"{before}\n  AND {condition}\n{after}" if after else f"{before}\n  AND {condition}"

    # No WHERE in the outermost query: insert WHERE <condition> before next clause (or end)
    next_clause_rel = None
    for kw in clause_keywords:
        idx = tail_lower.find(kw)
        if idx != -1 and (next_clause_rel is None or idx < next_clause_rel):
            next_clause_rel = idx

    insert_pos = len(sql) if next_clause_rel is None else (from_idx + next_clause_rel)
    before = sql[:insert_pos].rstrip()
    after = sql[insert_pos:].lstrip()
    return f"{before}\nWHERE {condition}\n{after}" if after else f"{before}\nWHERE {condition}"



def run_analysis(
    *,
    question,
    session,
    request_id=None,
    call_planner,
    planner_ctx, 
    extract_metric_manifest,
    extract_all_sql,
    validate_metric_sql_binding,
    run_all_bigquery,
    build_metric_payload,
    call_interpreter,
    interpreter_ctx,
    get_chat_history,
    append_chat_turn
):
    """
    Executes the full planner → SQL → BQ → interpreter pipeline.
    """

    if request_id is None:
        request_id = str(uuid.uuid4())

    t0 = time.perf_counter()

    planner_text = call_planner(
        question=question,
        chat_history=get_chat_history(),
        **planner_ctx
    )
    
    # ===== STEP 4: HARD FAIL ON INVALID PLANNER OUTPUT =====
    
    if planner_text.count("BEGIN_METRIC_MANIFEST") != 1:
        raise RuntimeError("Planner must return exactly ONE METRIC_MANIFEST block")
    
    if "BEGIN_SQL_BLOCK_1" not in planner_text:
        raise RuntimeError("Planner returned no SQL blocks")
    
    # Disallow legacy fenced SQL
    if "```" in planner_text:
        raise RuntimeError("Legacy SQL fences detected; use SQL block markers only")
    
    # Disallow SQL outside SQL blocks (basic guard)
    before_sql_blocks = planner_text.split("BEGIN_SQL_BLOCK_1", 1)[0]
    if "SELECT" in before_sql_blocks.upper():
        raise RuntimeError("SQL detected outside SQL block markers")
    
    # ===== END STEP 4 CHECKS =====

    planner_ms = int((time.perf_counter() - t0) * 1000)
    
    logger.info(
        "phase=planner | request_id=%s | duration_ms=%d",
        request_id,
        planner_ms
    )
    
    metric_manifest = extract_metric_manifest(planner_text)
    validate_manifest(metric_manifest)

    logger.info(
        "Metric manifest extracted | metric_count=%d | metric_ids=%s",
        len(metric_manifest),
        [m.get("metric_id") for m in metric_manifest]
    )
    # ---------------------------
    # SQL extraction + execution
    # ---------------------------
    sql_blocks = extract_all_sql(planner_text)
    validate_metric_sql_binding(metric_manifest, sql_blocks)

    # ---- Dead-hours enforcement (default ON) ----
    # Domain rule: exclude dead hours unless the USER explicitly asks to include them.
    # (We do NOT rely on the planner text for this decision.)
    exclude_dead_hours = should_exclude_dead_hours(question)

    # Keep logging of planner mention as observability only.
    logger.info(
        "dead_hours_policy | request_id=%s | exclude=%s | planner_mentions_dead_hours=%s",
        request_id,
        exclude_dead_hours,
        has_dead_hours_filter(planner_text),
    )

    if exclude_dead_hours:
        for q in sql_blocks:
            q["sql"] = apply_dead_hours_filter(q["sql"])

    t0 = time.perf_counter()
    raw_bq_results = run_all_bigquery(sql_blocks)
    bq_ms = int((time.perf_counter() - t0) * 1000)
    
    logger.info(
        "phase=bigquery | request_id=%s | duration_ms=%d | queries=%d",
        request_id,
        bq_ms,
        len(sql_blocks)
    )

    metric_payload = build_metric_payload(metric_manifest, raw_bq_results)

    metric = metric_manifest[0]
    sql_text = metric_payload[0]["results"][0]["sql"]

    filters = build_filters(metric, sql_text, planner_text)

    logger.info(
        "Metric payload built for interpreter | metric_count=%d",
        len(metric_payload)
    )

    logger.debug(
        "Metric payload | payload=%s",
        json.dumps(metric_payload, indent=2)
    )

    # ---------------------------
    # Interpreter
    # ---------------------------

    t0 = time.perf_counter()

    validated_answer = call_interpreter(
        question=question,
        planner_text=planner_text,
        metrics=metric_payload,
        **interpreter_ctx
    )

    assert isinstance(validated_answer, dict)

    interp_ms = int((time.perf_counter() - t0) * 1000)
    
    logger.info(
        "phase=interpreter | request_id=%s | duration_ms=%d",
        request_id,
        interp_ms
    )
    
    validate_answer(validated_answer)

    logger.info(
        "Interpreter output validated | metrics=%d",
        len(validated_answer["metrics"])
    )

    append_chat_turn(question, validated_answer.get("headline"))

    return {
        "question": question,
        "planner_text": planner_text,
        "filters": filters,
        "metric_manifest": metric_manifest,
        "sql_blocks": sql_blocks,
        "raw_bq_results": raw_bq_results,
        "metric_payload": metric_payload,
        "validated_answer": validated_answer,
        "request_id": request_id
    }
